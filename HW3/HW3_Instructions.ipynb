{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Classification, Evaluation, and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will have a chance to experience a complete machine learning cycle. You will prepare a dataset, make a model, evaluate models to find the best fit, and deploy it to a simple web page. Our main objective is to make students *learn* classification and evaluation methods, so we will apply essential data preprocessing techniques but mainly focus on classification and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **Pima Indians Diabetes Database** that is publicly available and from UCI. However, we removed and changed some parts of the dataset for the homework evaluation, so **please use the one in the zip file provided in ilearn**.\n",
    "\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on specific diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "According to the information on the data, it has eight attributes and one binary class. The brief explanation of the attributes are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pregnancies: Number of times pregnant.\n",
    "\n",
    "- Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test.\n",
    "\n",
    "- BloodPressure: Diastolic blood pressure (mm Hg).\n",
    "\n",
    "- SkinThickness: Triceps skin fold thickness (mm).\n",
    "\n",
    "- Insulin: 2-Hour serum insulin (mu U/ml).\n",
    "\n",
    "- BMI: Body mass index (weight in kg/(height in m)^2).\n",
    "\n",
    "- DiabetesPedigreeFunction: Diabetes pedigree function.\n",
    "\n",
    "- Age: Age (years)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we have a binary class which can be 0 (healthy) or 1 (diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Unlike the labs, each function you make here will be **graded**, so it is important to *strictly* follow input and output instruction stated in the skeleton code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Preparation\n",
    " - Load the dataset.\n",
    " - Task 1: Changing zero value into mean (not graded).\n",
    "1. Classification\n",
    " - Task 2: Random forest (graded, 0.5 pt).\n",
    " - Task 3: SVM with diverse kernels (graded, 0.5 pt).\n",
    " - Task 4: Decision tree implementation (graded, **advanced**, 2 pt).\n",
    "2. Evaluation\n",
    " - Task 5: Precision, Recall, F1-score (graded, 0.3 pt).\n",
    " - Task 6: AUC/AUPRC (graded, 0.2 pt).\n",
    " - Task 7: Apply them together with scikit-learn (graded, 0.5 pt).\n",
    " - Task 8: Task 5 implementation (graded, **advanced**, 1 pt).\n",
    "3. Deployment\n",
    " - Save models into a file using pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Student information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please provide your information for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUD_SUID = 'qiwa1131'\n",
    "STUD_NAME = 'Qiushi Wang'\n",
    "STUD_EMAIL = 'qiwa1131@student.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries will be frequently used throughout the homework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from HW3_helper import *\n",
    "RANDOM_STATE = 12345 #Do not change it!\n",
    "np.random.seed(RANDOM_STATE) #Do not change it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **diabetes** dataset located ilearn, and load it here using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"datasets/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find out some basic information by calling *info(), head()*, and *describe()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 9 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \ndtypes: float64(2), int64(7)\nmemory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is no null data. However, if you check zero values in the dataset, there are so many of them. Is it normal that people's BMI is zero? or not? You may want to change the zero values into another reasonable value, such as mean or median. The only thing that can have zero value is **pregnancies**. Let's first make a function changing zero values into the mean of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Changing zero value into mean (not graded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(df, columns):\n",
    "    \n",
    "    \"\"\"\n",
    "     A function to change nan value (or zero value) to the mean of the attribute\n",
    "        \n",
    "        - Step 1: Get a part of dataframe using columns received as a parameter.\n",
    "        - Step 2: Change the zero values in the columns to np.nan.\n",
    "        - Step 3: Change the nan values to the mean of each attribute (column). \n",
    "                  You can use apply() or fillna() function.\n",
    "        Input:\n",
    "          df: A dataframe that needs to apply imputation\n",
    "          columns: A list of columns that need to apply imputation\n",
    "        Output:\n",
    "          An imputed dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    #step 1 \n",
    "    \n",
    "    for col in columns:\n",
    "        col_values = df[col]\n",
    "        col_values = col_values.replace(0,np.nan)\n",
    "        df [col] = col_values\n",
    "  \n",
    "    for col in columns:\n",
    "        col_mean = df[col].mean()\n",
    "        df [col].fillna(value = col_mean)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_test = imputation(diabetes, [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing a simple data processing, let's proceed to our main task, classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to skip this part, you may want to use the imputation code from scikit-learn. If you are interested in imputation, you can find more information [here](https://scikit-learn.org/stable/modules/impute.html).\n",
    "**NOTE**: The imputation function itself is not graded, but it is required for you to run imputation as it will affect other functions' results that are graded. \n",
    "\n",
    "<span style=\"color:blue\"> **You HAVE TO run imputation even though it is not graded for the next tasks. It is your responsibliity to follow the instruction. If you want to skip this part, remove the comments below and run the code th perform imputation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  121.686763      72.405184      29.153420  155.548223   \n",
       "std       3.369578   30.435949      12.096346       8.790942   85.021108   \n",
       "min       0.000000   44.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.750000      64.000000      25.000000  121.500000   \n",
       "50%       3.000000  117.000000      72.202592      29.153420  155.548223   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  155.548223   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    32.457464                  0.471876   33.240885    0.348958  \n",
       "std      6.875151                  0.331329   11.760232    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.500000                  0.243750   24.000000    0.000000  \n",
       "50%     32.400000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.845052</td>\n      <td>121.686763</td>\n      <td>72.405184</td>\n      <td>29.153420</td>\n      <td>155.548223</td>\n      <td>32.457464</td>\n      <td>0.471876</td>\n      <td>33.240885</td>\n      <td>0.348958</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.369578</td>\n      <td>30.435949</td>\n      <td>12.096346</td>\n      <td>8.790942</td>\n      <td>85.021108</td>\n      <td>6.875151</td>\n      <td>0.331329</td>\n      <td>11.760232</td>\n      <td>0.476951</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>44.000000</td>\n      <td>24.000000</td>\n      <td>7.000000</td>\n      <td>14.000000</td>\n      <td>18.200000</td>\n      <td>0.078000</td>\n      <td>21.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>99.750000</td>\n      <td>64.000000</td>\n      <td>25.000000</td>\n      <td>121.500000</td>\n      <td>27.500000</td>\n      <td>0.243750</td>\n      <td>24.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>117.000000</td>\n      <td>72.202592</td>\n      <td>29.153420</td>\n      <td>155.548223</td>\n      <td>32.400000</td>\n      <td>0.372500</td>\n      <td>29.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.000000</td>\n      <td>140.250000</td>\n      <td>80.000000</td>\n      <td>32.000000</td>\n      <td>155.548223</td>\n      <td>36.600000</td>\n      <td>0.626250</td>\n      <td>41.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>17.000000</td>\n      <td>199.000000</td>\n      <td>122.000000</td>\n      <td>99.000000</td>\n      <td>846.000000</td>\n      <td>67.100000</td>\n      <td>2.420000</td>\n      <td>81.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Remove the comments and run the code below if you skip the parts above \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "columns = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "df_parts = diabetes.copy()[columns]\n",
    "df_parts[df_parts==0] = np.nan\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df_converted = pd.DataFrame(imp.fit_transform(df_parts), columns=columns)\n",
    "diabetes[columns] = df_converted\n",
    "diabetes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that the columns' min values are changed except for pregnancies' one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will try to run random forest (RF), and support vector machine (SVM) with different kernels using scikit-learn. As an extra task, we also have a chance to understand the decision tree in detail by implementing it from scratch. We will continue to use **the pre-processed diabetes dataset**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Random forest (graded, 0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will run the random forest algorithm using scikit-learn, together with cross-validation. Detailed information about the random forest in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "Your task is as follows:\n",
    " 1. Create a random forest classifier with the random state stated above (RANDOM_STATE).\n",
    " 2. Report an average cross-validation score with stratified k-fold with **k=5** into the variable called **rf_cross_val_score** (0.2 pt).\n",
    " 3. Run grid search with a dictionary having two elements; `max_depth` from 1 to 10, and `min_samples_split` from 2 to 10. Report the best classifier (or the best estimator) into the variable called **rf_best_classifier**. Use the same random forest classifier instance with the random state. Set **k=5** for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put a complete dataset, not a split training set (0.3 pt).\n",
    " \n",
    "* For clarification, **Task 2** and **Task 3** are independent. You have to run stratified k-fold for **Task 2**, but it will not be used in **Task 3**.\n",
    "* There is no further partial point for each subtask, so please be careful to read the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries if needed.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=RANDOM_STATE) \n",
    "# CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diabetes.columns = np.arange(9)\n",
    "X = diabetes.drop('Outcome',axis=1)\n",
    "y = diabetes.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  121.686763      72.405184      29.153420  155.548223   \n",
       "std       3.369578   30.435949      12.096346       8.790942   85.021108   \n",
       "min       0.000000   44.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.750000      64.000000      25.000000  121.500000   \n",
       "50%       3.000000  117.000000      72.202592      29.153420  155.548223   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  155.548223   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  768.000000                768.000000  768.000000  \n",
       "mean    32.457464                  0.471876   33.240885  \n",
       "std      6.875151                  0.331329   11.760232  \n",
       "min     18.200000                  0.078000   21.000000  \n",
       "25%     27.500000                  0.243750   24.000000  \n",
       "50%     32.400000                  0.372500   29.000000  \n",
       "75%     36.600000                  0.626250   41.000000  \n",
       "max     67.100000                  2.420000   81.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n      <td>768.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.845052</td>\n      <td>121.686763</td>\n      <td>72.405184</td>\n      <td>29.153420</td>\n      <td>155.548223</td>\n      <td>32.457464</td>\n      <td>0.471876</td>\n      <td>33.240885</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.369578</td>\n      <td>30.435949</td>\n      <td>12.096346</td>\n      <td>8.790942</td>\n      <td>85.021108</td>\n      <td>6.875151</td>\n      <td>0.331329</td>\n      <td>11.760232</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>44.000000</td>\n      <td>24.000000</td>\n      <td>7.000000</td>\n      <td>14.000000</td>\n      <td>18.200000</td>\n      <td>0.078000</td>\n      <td>21.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>99.750000</td>\n      <td>64.000000</td>\n      <td>25.000000</td>\n      <td>121.500000</td>\n      <td>27.500000</td>\n      <td>0.243750</td>\n      <td>24.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.000000</td>\n      <td>117.000000</td>\n      <td>72.202592</td>\n      <td>29.153420</td>\n      <td>155.548223</td>\n      <td>32.400000</td>\n      <td>0.372500</td>\n      <td>29.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.000000</td>\n      <td>140.250000</td>\n      <td>80.000000</td>\n      <td>32.000000</td>\n      <td>155.548223</td>\n      <td>36.600000</td>\n      <td>0.626250</td>\n      <td>41.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>17.000000</td>\n      <td>199.000000</td>\n      <td>122.000000</td>\n      <td>99.000000</td>\n      <td>846.000000</td>\n      <td>67.100000</td>\n      <td>2.420000</td>\n      <td>81.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "763    0\n",
       "764    0\n",
       "765    0\n",
       "766    1\n",
       "767    0\n",
       "Name: Outcome, Length: 768, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "scores_skf = cross_val_score(rf, X, y, cv=skf) \n",
    "rf_cross_val_score = np.mean(scores_skf) # CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7656735421441304"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "rf_cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'max_depth': [1,2,3,4,5,6,7,8,9,10],'min_samples_split': [2,3,4,5,6,7,8,9,10]},\n",
    " ]\n",
    "\n",
    "search = GridSearchCV(rf, param_grid,cv=5)\n",
    "search.fit(X, y)\n",
    "\n",
    "rf_best_classifier = search.best_estimator_\n",
    "# CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, min_samples_split=6, random_state=12345)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "rf_best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: SVM with diverse kernels (graded, 0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already tried a simple SVC with the RBF kernel before. Here you will rerun SVM, but trying different kernels, together with cross-validation. Detailed information about SVC in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).\n",
    "\n",
    "Your task is as follows:\n",
    "\n",
    "  1. Create a standard SVC classifier without setting any parameter.\n",
    "  2. You may want to re-scale the dataset so that all the attributes have the same range of values. Apply StandardScaler without changing any parameters. You can find information [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). *Please do not apply Scaler to the label* (0.1 pt).\n",
    "  3. Report test score of SVC model with **holdout test** with **test set ratio = 30%** into the variable called svm_ho_score. It means that you train the model using the training dataset and report the score using the test set. Since test_train_split function shuffles the dataset, do not forget to put the random state stated above (`RANDOM_STATE`) (0.2 pt).\n",
    "  4. Run grid search with a dictionary stating kernels ['linear', 'poly', 'rbf'] and C = [1, 10, 100], and put the best classifier into the variable called svm_best_classifier. Set **k=10** for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put the complete dataset, not split training set (0.2 pt).\n",
    "  \n",
    "  \n",
    "  * For clarification, **Task 3** and **Task 4** are independent. Any produced work in **Task 3** will not be used in **Task 4**.\n",
    "  * There is no further partial point for each subtask, so please be careful to read the instruction. Failing to apply StandardScaler affects the scores of part 3 and 4, as those are automatically graded, so be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to change the datasets here\n",
    "sd = StandardScaler()\n",
    "X = sd.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "svc = SVC() # CHANGE IT\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_ho_score = svc.score(X_test,y_test)\n",
    "# CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8268398268398268"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "svm_ho_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ = [{'C': [1,10,100],'kernel':['linear','poly','rbf']}]\n",
    "\n",
    "search_ = GridSearchCV(SVC(),param_grid_,cv=10)\n",
    "search_.fit(X, y)\n",
    "\n",
    "svm_best_classifier = search_.best_estimator_ # CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(C=1, kernel='linear')"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "svm_best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Decision tree implementation (graded, advanced, 2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is extra for those who want to get extra points! We will now implement a decision tree from scratch. Follow the instruction carefully so that you can return the correct result, which will be a criterion to grade. We will also offer a simple test function so you can validate your implementation.\n",
    "\n",
    "We have two different grading options:\n",
    "\n",
    "  - 4-1. Implement a decision tree without any constraints (1 pt)\n",
    "  - 4-2. Allow two main parameters: max_depth (0.5 pt) and min_size (0.5 pt) (in total 1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see our structure. Unlike labs, since this task is graded, we did not offer you class structure since it can make additional confusion to some students. We have seven separate methods, and here is a brief description of each method:\n",
    "\n",
    "- **dt_fit**: This function is first called with the dataset and creates the tree's root node. It also calls a recursive function to grow the tree.\n",
    "- **dt_score**: A function returning the accuracy scores of the received dataset and labels.\n",
    "- **dt_predict**: A recursive function that predict a row's label by going through the trained tree.\n",
    "- **find_best_split**: This function examines the best split by trying to split based on each attribute and a specific value.\n",
    "- **gini_index**: This function receives two groups (left, right) and calculates a Gini index of these two groups based on outcome distribution.\n",
    "- **leaf_final_value**: This function receives one group and returns the most common label (outcome) so that the tree can terminate with its final decision.\n",
    "- **split**: This function is a recursive function that calculates the best split and splits the node into two parts until specific criteria are met, such as minimum samples or max depth of the tree.\n",
    "\n",
    "\n",
    "\n",
    "* Unfortunately, there is no further partial point for each subtask, so please be careful to read the instruction.\n",
    "* Part 2 (4-2) is only counted when you successfully finish part 1 (4-1). So prioritize finishing 4-1 first to get scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#series = pd.Series(['g', 'e', 'e', 'k', 's','f', 'o', 'r', 'g', 'e', 'e', 'k', 's']) \n",
    "#print(\"Printing the Original Series:\") \n",
    "#display(series) \n",
    "  \n",
    "# counting the frequency of each element \n",
    "#freq = series.value_counts().index [0]\n",
    "#print(\"Printing the frequency\") \n",
    "#display(freq) \n",
    "#freq = y.value_counts().index [0]\n",
    "#freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_fit(X, y, min_samples_split=1, max_depth=np.inf):\n",
    "#def dt_fit(X, y, min_samples_split, max_depth):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      X: Training dataset.\n",
    "      y: Training labels.\n",
    "      min_samples_split: constraint. Minimum number of samples in the node that the algorithm stops splitting.\n",
    "      max_depth: constraint. Maximum number of depth from the root that the algorithm stops splitting.\n",
    "      * X and y should have the same size.\n",
    "    Output:\n",
    "      root: A root node having the whole information of the tree after completing recursion.\n",
    "    \"\"\"\n",
    "    # the data structure of a node was designed as \n",
    "    # [chosen_column,chosen_value,[child_node_left,child_node_right]], and it is the only datastructure of a node, no matter the node is a root node or a child node.\n",
    "    # the program always follow the rule: firstly use find_best_split() to a node, then use split() to its child nodes. \n",
    "    root_node = find_best_split(X,y)\n",
    "    child_L = root_node [2][0] #[Left_X,Left_y]\n",
    "    child_R = root_node [2][1] #[Right_X,Right_y]\n",
    "    next_L = find_best_split(child_L[0],child_L[1])\n",
    "    next_R = find_best_split(child_R[0],child_R[1])\n",
    "\n",
    "    # current node, and two children.\n",
    "    return [root_node[0],root_node[1],[split(next_L,0, min_samples_split, max_depth),split(next_R,0, min_samples_split, max_depth)]]\n",
    "    #return\n",
    "\n",
    "\n",
    "\n",
    "def find_best_split(X, y):\n",
    "\n",
    "    '''\n",
    "    - Step 1: Get possible unique labels of the current node\n",
    "    - Step 2: Iterate each column and the possible unique values of each column (double loops),\n",
    "              and try dividing a node into two parts by the specific value of the chosen column (the current two values of the loop).\n",
    "    - Step 3: Calculate a gini index of the node with the separated parts by chosen column and value.\n",
    "              Since we are dealing with continuous values, divide the datasets with the following criteria:\n",
    "               - if the value of the chosen column is lower than the chosen value -> assign it to the left node\n",
    "               - otherwise (higher or equal to) -> assign it to the right node.\n",
    "               - Then we can call gini_index function with those two nodes' information.\n",
    "    - Step 4: By calling gini_index function for every (column, value) pair,\n",
    "              get the best gini index by iterating all the values and columns from the dataset that the node has.\n",
    "    - Step 5: With chosen criteria, create a node structure with the following information: [column, value to split, children]\n",
    "              It is up to you to specify the structure, but here is an example.\n",
    "              {'index': A chosen column name having the best gini index score,\n",
    "               'value': A chosen value in the index column having the best gini index score,\n",
    "               'children': A list that contains splitted groups [left, right]}.\n",
    "    '''\n",
    "    # step 1: unique labels\n",
    "    unique_labels = y.unique()\n",
    "    col_gini = 1.1\n",
    "    lowest_gini_of_columns = ''\n",
    "    split_value = 0\n",
    "    # loop for column, '1st loop in dobule loop'\n",
    "    for col in X:\n",
    "        # possible uniques values for current column\n",
    "        possible_unique_values = X [col].unique()\n",
    "        # I concat class label with corresponding value for convenience of calculating gini index\n",
    "        col_with_label = pd.concat([X [col], y], axis=1)\n",
    "        col_with_label.columns = ['col','label']\n",
    "        gini_ = 1.1\n",
    "        chosen_value = 0\n",
    "        #loop possible unique values for current column, '2nd loop in double loop'\n",
    "        for value in possible_unique_values:\n",
    "            left_ = col_with_label.iloc[lambda x: x['col'].values < value]\n",
    "            right_ = col_with_label.iloc[lambda x: x['col'].values >= value]\n",
    "            g_i = gini_index([left_,right_], unique_labels)\n",
    "            if g_i < gini_:\n",
    "                gini_ = g_i\n",
    "                chosen_value = value\n",
    "        # if gini index from the latest column is less than previous lowest value, then update information.\n",
    "        if gini_<col_gini:\n",
    "            col_gini = gini_\n",
    "            split_value = chosen_value\n",
    "            lowest_gini_of_columns = col\n",
    "\n",
    "   \n",
    "    # prepare result\n",
    "    Left_X = X.iloc[lambda x: x[lowest_gini_of_columns].values < split_value]\n",
    "    Right_X = X.iloc[lambda x: x[lowest_gini_of_columns].values >= split_value]\n",
    "    Left_y = y[Left_X.index]\n",
    "    Right_y = y[Right_X.index]\n",
    "\n",
    "    Left = [Left_X,Left_y]\n",
    "    Right = [Right_X,Right_y]\n",
    "    Children = [Left,Right]\n",
    "    result = [lowest_gini_of_columns,split_value,Children]\n",
    "    return result\n",
    "    \n",
    "\n",
    "def gini_index(children, classes):\n",
    "    \"\"\"\n",
    "    Input\n",
    "      children: A list that contains splitted groups [left, right]}.\n",
    "      classes: Possible outcomes of the part of dataset.\n",
    "    Output\n",
    "      A gini index value.\n",
    "    \"\"\" \n",
    "    # get size of children\n",
    "    children_size = 0\n",
    "    gini_index = 0\n",
    "    # count the size of children\n",
    "    for child in children:\n",
    "        children_size += child.shape [0]\n",
    "\n",
    "    # for each child\n",
    "    for child in children:\n",
    "        child_size = child.shape [0]\n",
    "        if child_size == 0:\n",
    "            continue;\n",
    "\n",
    "        child_score = 0\n",
    "        # for each label of a child, count child score\n",
    "        for label in classes:\n",
    "            #label_count = 0\n",
    "            # iterate each row in child, to count label\n",
    "            '''\n",
    "            for index,line in child.iterrows():\n",
    "                if line[-1] == label:\n",
    "                    label_count += 1\n",
    "            '''\n",
    "            label_count = sum(child.iloc[lambda x: x['label'].values == label]['label'])\n",
    "            \n",
    "            proportion = label_count / child_size \n",
    "            sqaured_p = np.square(proportion)\n",
    "            child_score += sqaured_p \n",
    "\n",
    "        child_gini = (1 - child_score) * (child_size/children_size)\n",
    "        gini_index += child_gini\n",
    "    \n",
    "    # end for child in children\n",
    "    return gini_index\n",
    "\n",
    "\n",
    "def leaf_final_value(y):\n",
    "    \"\"\"\n",
    "    A function that returns the most common label given labels in a specific node.\n",
    "    Input\n",
    "      y: A list of labels of the part of dataset.\n",
    "    Output\n",
    "      The most common label in the input series.\n",
    "    \"\"\"\n",
    "    if y.shape [0] == 0:\n",
    "        return\n",
    "    else:\n",
    "        return [y.value_counts().index [0]]\n",
    "\n",
    "\n",
    "#def split(node, depth, min_samples_split, max_depth):\n",
    "def split(node, depth, min_samples_split=1, max_depth=np.inf):\n",
    "    \"\"\"\n",
    "    A recursive function to split the node into two parts based on [the result from find_best_split function].\n",
    "    This function will create left and right children in the node structure (so the current node will have all its children).\n",
    "    \n",
    "    If you only developed part 4-1, just follow Step 1 and Step 4.\n",
    "\n",
    "    - Step 1: Termination 1: Check whether the size of left and right child is zero.\n",
    "              If so, call leaf_final_value for **both children** to finalize the node.\n",
    "    - Step 2: Termination 2: If the depth of current node reaches the maximum depth parameter (max_depth),\n",
    "              again call leaf_final_value to finalize the node.\n",
    "    \n",
    "    * Step 3-4 should be applied to each child separately.\n",
    "    - Step 3: Termination 3: If the number of samples in the left or right node is smaller than our threshold (min_samples_split),again call leaf_final_value to finalize the corresponding child (left or right).\n",
    "\n",
    "    \"\"\"\n",
    "    #print('\\n depth: ',depth)\n",
    "    # empty\n",
    "    if len(node) == 0:\n",
    "        return\n",
    "    # a label\n",
    "    if len(node) == 1:\n",
    "        return node\n",
    "\n",
    "    Children = node [2]\n",
    "    Left = Children [0]\n",
    "    Right = Children [1]\n",
    "    Left_X = Left [0]\n",
    "    Left_y = Left [1]\n",
    "    Right_X = Right [0]\n",
    "    Right_y = Right [1]\n",
    "\n",
    "    #Termination 2: If the depth of current node reaches the maximum depth parameter (max_depth). again call leaf_final_value to finalize the node.\n",
    "    \n",
    "    if depth >= max_depth:\n",
    "        #print('max_depth works here')\n",
    "        y_ = Left_y.append(Right_y)\n",
    "        return leaf_final_value(y_)\n",
    "    \n",
    "    #Termination 3: If the number of samples in the left or right node is smaller than our threshold (min_samples_split), again call leaf_final_value to finalize the corresponding child (left or right).\n",
    "    \n",
    "    if Left_X.shape[0]<min_samples_split:\n",
    "        #print('min_samples_split works here on the left')\n",
    "        child_node_L = leaf_final_value(Left_y)\n",
    "    if Right_X.shape[0]<min_samples_split:\n",
    "        #print('min_samples_split works here on the right')\n",
    "        child_node_R = leaf_final_value(Right_y)\n",
    "    \n",
    "    #Termination 1: Check whether the size of left and right child is zero. if so, call leaf_final_value for **both children** to finalize the node.\n",
    "    if Left_X.empty == True and Right_X.empty == True:\n",
    "        return\n",
    "    if Left_X.empty == True:\n",
    "        child_node_L = []\n",
    "        child_node_R = leaf_final_value(Right_y)\n",
    "    elif Right_X.empty == True:\n",
    "        child_node_R = []\n",
    "        child_node_L = leaf_final_value(Left_y)\n",
    "    # continue spliting    \n",
    "    else:\n",
    "        child_node_L = find_best_split(Left_X,Left_y)\n",
    "        child_node_R = find_best_split(Right_X,Right_y)\n",
    "\n",
    "        #depth = depth+1\n",
    "    #return: [ (1)current_node_chosen_attribute_for_split, (2)current_node_chosen_value_for_split, (3)[split(Left_Child),split(Right_child)]]\n",
    "    return [node[0],node[1],[split(child_node_L, depth+1, min_samples_split, max_depth),split(child_node_R, depth+1, min_samples_split, max_depth) ]]\n",
    "\n",
    "def dt_score(tree, X, y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      tree: A trained tree returned by fit function.\n",
    "      X: A test dataset.\n",
    "      y: Test labels.\n",
    "    Output:\n",
    "      An accuracy score.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    length = y.shape[0]\n",
    "    \n",
    "    idx = np.arange(length)\n",
    "    y_pred = []\n",
    "    for i in idx:\n",
    "        current_row = X.iloc[[i]]\n",
    "        y_pred.append(dt_predict(tree,current_row))\n",
    "\n",
    "\n",
    "    score = accuracy_score(y, y_pred)\n",
    "    return score\n",
    "    \n",
    "def dt_predict(node, row):\n",
    "    \"\"\"\n",
    "    A recursive function that predict a row's label by going through the trained tree.\n",
    "    Input:\n",
    "      node: A current node to check for splitting.\n",
    "      row: A single row in a dataset.\n",
    "    Output:\n",
    "      A predicted label.\n",
    "    \"\"\"\n",
    "    # predict a false label if node = None\n",
    "    if node == None:\n",
    "        return 0\n",
    "\n",
    "    length = len(node)\n",
    "    row = row.reset_index(drop=True)\n",
    "    #next_node = NULL\n",
    "    next_branch = 1 # default right branch\n",
    "    if length == 3: # [A,S,[Left,Right]]\n",
    "        attr = node [0]\n",
    "        value = row [attr][0]\n",
    "        \n",
    "        if value < node [1]:\n",
    "            next_branch = 0 # go left branch\n",
    "        \n",
    "        next_node = node [2][next_branch]  # next node\n",
    "        # if a next node is empty, it means ending, like [leftNode,[]] or [[],rightNode]. so go to the other node. \n",
    "        if next_node == None:\n",
    "            return dt_predict(node [2][(-1)*next_branch],row)\n",
    "\n",
    "        return dt_predict(next_node,row)    \n",
    "    elif length == 1: # give the label\n",
    "        return node [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset for decision tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "test_tree = pd.read_csv(\"datasets/diabetes.csv\")\n",
    "columns = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "df_parts = test_tree.copy()[columns]\n",
    "df_parts[df_parts==0] = np.nan\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "df_converted = pd.DataFrame(imp.fit_transform(df_parts), columns=columns)\n",
    "test_tree[columns] = df_converted\n",
    "test_tree.describe()\n",
    "test_tree.head()\n",
    "#test_tree = test_tree.iloc[100:130,:]\n",
    "tree_X = test_tree.drop(['Outcome'],axis=1)\n",
    "tree_y = test_tree.iloc[:, -1]\n",
    "\n",
    "#train data, test data\n",
    "X_tree_train, X_tree_test, y_tree_train, y_tree_test = train_test_split(tree_X, tree_y, test_size = 0.3, random_state=RANDOM_STATE,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before fit: Sun Oct 25 01:01:32 2020 \n",
      "\n",
      "after fit:  Sun Oct 25 01:02:14 2020 \n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7445887445887446"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# test my own tree\n",
    "import time\n",
    "before = time.asctime(time.localtime(time.time()))\n",
    "print('before fit:',before,'\\n')\n",
    "Tree = dt_fit(X_tree_train, y_tree_train, min_samples_split=3, max_depth=6)\n",
    "after = time.asctime(time.localtime(time.time()))\n",
    "print('after fit: ',after,'\\n')\n",
    "#print('time comsumption: ',after - before)\n",
    "score = dt_score(Tree,X_tree_test,y_tree_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Glucose',\n",
       " 155.0,\n",
       " [['Age',\n",
       "   29.0,\n",
       "   [['SkinThickness',\n",
       "     29.0,\n",
       "     [['Age',\n",
       "       27.0,\n",
       "       [['Pregnancies',\n",
       "         4,\n",
       "         [['Pregnancies',\n",
       "           2,\n",
       "           [['Pregnancies', 0, [None, [0]]], ['Pregnancies', 2, [None, [0]]]]],\n",
       "          ['Pregnancies', 4, [None, [0]]]]],\n",
       "        ['Insulin',\n",
       "         190.0,\n",
       "         [['DiabetesPedigreeFunction',\n",
       "           0.867,\n",
       "           [['Pregnancies', 4, [[0], [0]]],\n",
       "            ['BloodPressure', 68.0, [[1], [0]]]]],\n",
       "          ['Pregnancies', 3, [None, [1]]]]]]],\n",
       "      ['BMI',\n",
       "       52.9,\n",
       "       [['Glucose',\n",
       "         128.0,\n",
       "         [['DiabetesPedigreeFunction',\n",
       "           0.503,\n",
       "           [['Glucose', 112.0, [[0], [0]]], ['Insulin', 100.0, [[1], [0]]]]],\n",
       "          ['BloodPressure',\n",
       "           75.0,\n",
       "           [['BloodPressure', 66.0, [[0], [1]]],\n",
       "            ['Pregnancies', 4, [[0], [1]]]]]]],\n",
       "        ['Pregnancies', 0, [None, [1]]]]]]],\n",
       "    ['BMI',\n",
       "     26.5,\n",
       "     [['Age',\n",
       "       60.0,\n",
       "       [['Pregnancies',\n",
       "         2,\n",
       "         [['Pregnancies', 1, [None, [0]]], ['Pregnancies', 2, [None, [0]]]]],\n",
       "        ['DiabetesPedigreeFunction',\n",
       "         0.409,\n",
       "         [['Pregnancies', 4, [None, [1]]],\n",
       "          ['Pregnancies',\n",
       "           8,\n",
       "           [['Pregnancies', 6, [[0], [0]]],\n",
       "            ['Pregnancies', 8, [None, [0]]]]]]]]],\n",
       "      ['Glucose',\n",
       "       97.0,\n",
       "       [['DiabetesPedigreeFunction',\n",
       "         1.224,\n",
       "         [['Pregnancies',\n",
       "           12,\n",
       "           [['Pregnancies', 5, [[0], [0]]], ['BMI', 32.8, [[1], [0]]]]],\n",
       "          ['Pregnancies', 5, [None, [1]]]]],\n",
       "        ['DiabetesPedigreeFunction',\n",
       "         0.516,\n",
       "         [['Glucose',\n",
       "           112.0,\n",
       "           [['BMI', 27.8, [[1], [0]]], ['BloodPressure', 68.0, [[1], [1]]]]],\n",
       "          ['BMI',\n",
       "           32.5,\n",
       "           [['Glucose', 129.0, [[1], [0]]],\n",
       "            ['DiabetesPedigreeFunction',\n",
       "             1.1740000000000002,\n",
       "             [[1], [0]]]]]]]]]]]]],\n",
       "  ['Age',\n",
       "   54.0,\n",
       "   [['Insulin',\n",
       "     680.0,\n",
       "     [['DiabetesPedigreeFunction',\n",
       "       0.344,\n",
       "       [['SkinThickness',\n",
       "         18.0,\n",
       "         [['Pregnancies', 3, [None, [0]]],\n",
       "          ['Insulin',\n",
       "           90.0,\n",
       "           [['Pregnancies', 1, [None, [0]]],\n",
       "            ['DiabetesPedigreeFunction', 0.13699999999999998, [[0], [1]]]]]]],\n",
       "        ['BloodPressure',\n",
       "         94.0,\n",
       "         [['Insulin',\n",
       "           545.0,\n",
       "           [['Age', 49.0, [[1], [1]]], ['Pregnancies', 5, [[1], [0]]]]],\n",
       "          ['Pregnancies',\n",
       "           4,\n",
       "           [['Pregnancies', 0, [None, [1]]],\n",
       "            ['Pregnancies', 6, [[0], [0]]]]]]]]],\n",
       "      ['Pregnancies', 0, [None, [0]]]]],\n",
       "    ['Glucose',\n",
       "     189.0,\n",
       "     [['BloodPressure',\n",
       "       110.0,\n",
       "       [['Pregnancies',\n",
       "         6,\n",
       "         [['Pregnancies', 0, [None, [0]]], ['Pregnancies', 6, [None, [0]]]]],\n",
       "        ['Pregnancies', 9, [None, [1]]]]],\n",
       "      ['Pregnancies',\n",
       "       8,\n",
       "       [['Pregnancies',\n",
       "         6,\n",
       "         [['Pregnancies', 1, [None, [1]]], ['Pregnancies', 6, [None, [1]]]]],\n",
       "        ['Pregnancies', 8, [None, [0]]]]]]]]]]]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7359307359307359"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# test DecisionTreeClassifier()\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_tree_train, y_tree_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "length = y_tree_test.shape[0]\n",
    "idx = np.arange(length)\n",
    "y_pred = []\n",
    "for i in idx:\n",
    "    current_row = X_tree_test.iloc[[i]]\n",
    "    y_pred.append(clf.predict(current_row))\n",
    "    \n",
    "\n",
    "#sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "score_sklearntree = accuracy_score(y_tree_test, y_pred)\n",
    "score_sklearntree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf\n",
    "#from sklearn import tree\n",
    "#tree.plot_tree(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Precision, Recall, F1-score (graded, 0.3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will evaluate the random forest and the support vector machine classifier with various performance measures besides accuracy such as precision, recall, and F1-score, also using scikit-learn. Here we continue to use the Pima Indians Diabetes Database dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "\n",
    "1. Scale the attributes in the dataset using *StandardScaler*. Please don't apply it to the labels.\n",
    "2. Create an instance of the SVC classifier without setting any constraint.\n",
    "3. Divide the dataset into two parts: a training set and a test set using the train_test_split method. Assign 30% of the dataset to the test set. \n",
    "  Please **turn off** shuffling the data.\n",
    "4. Fit the model using the training set.\n",
    "5. Report precision score (0.1 pt), recall score (0.1 pt), and F1-score (0.1 pt) using the test set, and save it into the variable called *recall_score_svc, precision_score_svc, f1_score_svc*. You can find out the information about the performance measures [here](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics). We require you to calculate the scores using the following functions: *precision_score, recall_score, f1_score*.\n",
    "\n",
    "\n",
    "* Scaling the data can affect the results and also affect your scores. Please be careful to follow the instruction.\n",
    "* There is no partial point except for the ones mentioned in part 5. There is also no any points if the result is incorrect, so you should correctly solve parts 1-4 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = diabetes.drop('Outcome',axis=1)\n",
    "y = diabetes.iloc[:, -1]\n",
    "clf = SVC()\n",
    "sd = StandardScaler()\n",
    "X = sd.fit_transform(X)\n",
    "# didn't mention random state in instruction\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30,shuffle=False)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test) \n",
    "\n",
    "recall_score_svc = recall_score(y_test, y_pred) \n",
    "precision_score_svc = precision_score(y_test, y_pred)\n",
    "f1_score_svc = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5569620253164557"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "recall_score_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7333333333333333"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "precision_score_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6330935251798561"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "f1_score_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: AUC / AUPRC (graded, 0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will evaluate the random forest and the support vector machine classifier with various performance measures related to the ROC curve, such as the area under the ROC curve (AUC) and rea under the precision-recall curve (AUPRC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "\n",
    "1. Create an instance of a random forest classifier without setting any constraint. Don't forget to set the random state to our value RANDOM_STATE.\n",
    "2. Divide the dataset into two parts: a training set and a test set using the train_test_split method. Assign 30% of the dataset to the test set. As the method will shuffle the data, please again set the random state to our value RANDOM_STATE.\n",
    "3. Fit the model using the training set. Please note that we no longer use scalaed dataset used in the previous task. Use the original dataset here.\n",
    "4. Report AUC (0.1 pt) and AUPRC (0.1 pt) using the test set, and save it into the variable called *auc_rf, auprc_rf*. You can find out the information about the performance measures [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for AUC score, and [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) for AUPRC score. AUPRC has many names, and it is supported as *average precision score* in scikit-learn. We require you to calculate the scores using the following functions: *roc_auc_score, average_precision_score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "X = diabetes.drop('Outcome',axis=1)\n",
    "y = diabetes.iloc[:, -1]\n",
    "rf_clf = RandomForestClassifier(random_state=RANDOM_STATE) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=RANDOM_STATE,shuffle=True)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "pred_y = rf_clf.predict(X_test)\n",
    "\n",
    "auc_rf =  roc_auc_score(y_test, pred_y) # CHANGE IT\n",
    "auprc_rf = average_precision_score(y_test, pred_y) # CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7649557829027224"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "auc_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5780967890556932"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "auprc_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: Apply them together with scikit-learn (graded, 0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will try to apply the grid search using the performance measures you have tried on Task 5 and Task 6, and pick the best performing model in terms of specific performance measures.\n",
    "\n",
    "Our dataset is imbalanced, meaning that the healthy patient is dominant. Therefore, we can expect that the best model can be different, and we may also need to use AUPRC to get the most suitable model. \n",
    "\n",
    "Your task is as follows:\n",
    "\n",
    "1. Create an instance of a kNN classifier without setting any constraint. \n",
    "2. Run grid search with a dictionary stating n_neighbors from 1 to 10, and use two different scoring measures: AUPRC (average_precision) and F1-score (f1). So you may want to run two different grid-search.\n",
    "3. Put the best classifiers into the respective variable called *auprc_best_classifier* (0.25 pt) and *f1_best_classifier* (0.25 pt). Set cv=5 for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put the complete dataset, not split training set.\n",
    "\n",
    "\n",
    "* Unfortunately, there is no further partial point for each subtask, so please be careful to read the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes.drop('Outcome',axis=1)\n",
    "y = diabetes.iloc[:, -1]\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=RANDOM_STATE,shuffle=True)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "param_grid_ = [{'n_neighbors': [1,2,3,4,5,6,7,8,9,10]}]\n",
    "\n",
    "search_auprc = GridSearchCV(clf_knn,param_grid_,cv=5,scoring='average_precision')\n",
    "search_auprc.fit(X, y)\n",
    "\n",
    "search_f_one = GridSearchCV(clf_knn,param_grid_,cv=5,scoring='f1')\n",
    "search_f_one.fit(X, y)\n",
    "\n",
    "#svm_best_classifier = search_.best_estimator_ # CHANGE IT\n",
    "\n",
    "auprc_best_classifier = search_auprc.best_estimator_\n",
    "f1_best_classifier = search_f_one.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=10)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "auprc_best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=9)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "f1_best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8: Task 5 implementation (graded, advanced, 1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extra task requires you to implement the following performance measures:\n",
    " - Accuracy (0.25 pt)\n",
    " - Precision (0.25 pt)\n",
    " - Recall (0.25 pt)\n",
    " - F1-score (0.25 pt)\n",
    " \n",
    "All inputs will be the NumPy arrays, so you can use any NumPy array methods to calculate the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_manual(predicted, truth):\n",
    "    # Write a logic and return accuracy\n",
    "    length = len(truth)\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    \n",
    "    index = np.arange(length)\n",
    "    for i in index:\n",
    "        if predicted [i]==1 and truth [i] == 1:\n",
    "            tp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 0:\n",
    "            tn += 1\n",
    "        elif predicted [i]==1 and truth [i] == 0:\n",
    "            fp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 1:\n",
    "            fn += 1\n",
    "    \n",
    "                \n",
    "\n",
    "    return (tp+tn)/(tp+tn+fp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_manual(predicted, truth):\n",
    "    # Write a logic and return precision\n",
    "    length = len(truth)\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    \n",
    "    index = np.arange(length)\n",
    "    for i in index:\n",
    "        if predicted [i]==1 and truth [i] == 1:\n",
    "            tp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 0:\n",
    "            tn += 1\n",
    "        elif predicted [i]==1 and truth [i] == 0:\n",
    "            fp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_manual(predicted, truth):\n",
    "    # Write a logic and return recall\n",
    "    length = len(truth)\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    \n",
    "    index = np.arange(length)\n",
    "    for i in index:\n",
    "        if predicted [i]==1 and truth [i] == 1:\n",
    "            tp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 0:\n",
    "            tn += 1\n",
    "        elif predicted [i]==1 and truth [i] == 0:\n",
    "            fp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 1:\n",
    "            fn += 1\n",
    "    return tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_manual(predicted, truth):\n",
    "    # Write a logic and return f1 score\n",
    "    length = len(truth)\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    \n",
    "    index = np.arange(length)\n",
    "    for i in index:\n",
    "        if predicted [i]==1 and truth [i] == 1:\n",
    "            tp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 0:\n",
    "            tn += 1\n",
    "        elif predicted [i]==1 and truth [i] == 0:\n",
    "            fp += 1\n",
    "        elif predicted [i]==0 and truth [i] == 1:\n",
    "            fn += 1\n",
    "    return (2*tp)/(2*tp+fp+fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you complete the method, you can run the following line to check whether your functions are correct or not. Note that we will evaluate your functions with different data, so please still be careful to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1. Accuracy test\nCorrect!\n2. Precision test\nCorrect!\n3. Recall test\nCorrect!\n4. F1-score test\nF1 test failed! You should have got 0.6666666666666667 but you got 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "check_scores(accuracy_manual, precision_manual, recall_manual, f1_score_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will learn how to pick the best model using cross-validation and deploy the best model as a file. This task will only be graded if you intend to do an extra task (HW 3.3), as loading the best model from this lab is one of the requirements of the next extra assignment. This part will be taught in Lab 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "\n",
    "1. Scale the attributes in the dataset using *StandardScaler*.\n",
    "2. Create an instance of an SVC classifier without setting any constraint.\n",
    "3. Run grid search with a dictionary stating a list of C values [1, 10, 100], and classifiers {'linear', 'poly', 'rbf'}. When examining 'poly' kernel, please also find the best classifier by testing degree = [2,3,4]. You may need to make more than one dictionary. Please use **AUPRC** as its scoring measure. Set cv=5 for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put the complete dataset, not split training set.\n",
    "4. Save the best classifier into the variable called *svm_best_classifier_2* and train this classifier using the whole dataset we have using the fit method inside the best classifier returned by grid search.\n",
    "5. Save the trained model using pickle and use this model as your deployed model for the Dash visualization. Detailed instruction can be found in Lab 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completing only this task will not be graded. To get one extra point, you need to use the **best model** from this task to show the **Dash** application. We will check the following points:\n",
    "\n",
    " 1) Whether the student successfully finds out the best classifier by following the instruction correctly.\n",
    " \n",
    " 2) Whether the student deploys the model successfully using the Dash framework with the given dataset.\n",
    " \n",
    "\n",
    "- It is highly recommmended to finish Lab 5 first before starting this section. You need to modify the files provided in Lab 5 (dash_example_web, helper_dash_example) to be appropriate for this task (that does not explicitly require the knowledge on HTML/Web programming), to handle different dataset having different columns and target label. \n",
    "- You do not need to change all the appearance but it should work with the new dataset in this homework and the best model derived in this task. It means that the deployed website should classify the new user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVC(C=100, degree=2, kernel='poly')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "X = diabetes.drop('Outcome',axis=1)\n",
    "y = diabetes.iloc[:, -1]\n",
    "clf = SVC()\n",
    "# do not have to standard\n",
    "#sd = StandardScaler()\n",
    "#X = sd.fit_transform(X)\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100], 'kernel': ['linear','rbf']},\n",
    "  {'C': [1, 10, 100], 'degree': [2,3,4], 'kernel': ['poly']},\n",
    " ]\n",
    "\n",
    "search = GridSearchCV(clf,param_grid,cv=5,scoring='average_precision')\n",
    "search.fit(X, y)\n",
    "\n",
    "svm_best_classifier_2 = search.best_estimator_ # CHANGE IT\n",
    "print(svm_best_classifier_2)\n",
    "#pred = svm_best_classifier_2.predict(X)\n",
    "#print(classification_report(y,pred))\n",
    "\n",
    "\n",
    "FOLDER_PATH = \"deployment/\"\n",
    "trained_model_filename = FOLDER_PATH + \"model.pickle\"\n",
    "\n",
    "data_to_save = svm_best_classifier_2\n",
    "file_path = trained_model_filename\n",
    "\n",
    "with open(file_path, \"wb\") as writeFile:\n",
    "    pickle.dump(data_to_save, writeFile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}